{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC1005 2018-2: Ayudantía Machine Learning\n",
    "\n",
    "La idea de esta ayudantía es guiar el preprocesamiento de datos que deben realizar para su tarea. Además, veremos reducción de dimensionalidad y clustering en un set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "alt.renderers.enable('notebook')\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar cargamos datos que vienen con **sklearn**, estos son los datos MNIST, un dataset de dígitos, cuyos datos representan a una matriz de 8x8 con un dibujo de un número y una label que representa al numero mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = (pd.DataFrame(digits['data']), pd.DataFrame(digits['target']))\n",
    "\n",
    "data = pd.DataFrame(normalize(data))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos t-SNE, un algoritmo de reducción de dimensionalidad para mapear cada vector de dimensión 64 a otro de dimensión 2.\n",
    "\n",
    "t-SNE funciona de modo que los puntos que se encuentran cercanos en el espacio de 64 dimensiones también lo esten en 2 dimensiones y lo mismo para puntos que se encuentran lejanos.\n",
    "\n",
    "En el ejemplo se agrega también una letra, sin embargo esto sólo es para que altair nos muestre cada punto distinto con un color correspondiente a la label, lo que hace que todos los números iguales se muestren de un mismo color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color(num):\n",
    "    return chr(num + 66)\n",
    "\n",
    "ts = TSNE(n_components=2).fit_transform(data)\n",
    "tsne = pd.DataFrame(ts, columns=['x', 'y'])\n",
    "tsne['label'] = targets.applymap(lambda x: color(x))\n",
    "tsne.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que poseemos vectores de sólo dos dimensiones, podemos usar Altair para plotearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(tsne).mark_point().encode(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    color='label'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos DBSCAN, un algoritmo de **clustering** para crear clusters de los puntos. Este algoritmo crea clusters a partir de la densidad de los puntos. Es decir que un conjunto de puntos cercanos formarán parte del mismo cluster, mientras que los puntos mas alejados no serán parte de él.\n",
    "\n",
    "El parámetro que recibe DBSCAN es **eps**, que representa la distancia máxima que pueden tener dos puntos que están en el mismo cluster y **min\\_samples**, que representa la cantidad de puntos que debe tener a su alrededor otro punto para ser considerado como **punto central** de un cluster.\n",
    "\n",
    "Igual que en el ejemplo anterior, usamos letras para colorear los puntos, sin embargo estas no dependen de las labels originales de los datos, sino que de los clusters entregados por el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=7, min_samples=5).fit(ts)\n",
    "\n",
    "cluster_labels = pd.DataFrame(data=clustering.labels_)\n",
    "\n",
    "dbscan = pd.DataFrame(ts, columns=['x', 'y'])\n",
    "\n",
    "dbscan['label'] = cluster_labels.applymap(lambda x: color(x))\n",
    "\n",
    "dbscan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(dbscan).mark_point().encode(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    color='label'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que pasa si bajamos **eps** de 7 a 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=4, min_samples=5).fit(ts)\n",
    "\n",
    "cluster_labels = pd.DataFrame(data=clustering.labels_)\n",
    "\n",
    "dbscan = pd.DataFrame(ts, columns=['x', 'y'])\n",
    "\n",
    "dbscan['label'] = cluster_labels.applymap(lambda x: color(x))\n",
    "\n",
    "dbscan.head()\n",
    "\n",
    "alt.Chart(dbscan).mark_point().encode(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    color='label'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, volveremos a nuestros datos originales de dimensión 64 para entrenar clasificadores.\n",
    "\n",
    "En primer lugar, separaremos los datos 70%/30%. Usualmente mezclamos los datos, pero esta vez no lo haremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = round(len(digits['data']) * 0.7)\n",
    "training_data, training_labels = (digits['data'][:amount], digits['target'][:amount])\n",
    "test_data, test_labels = (digits['data'][amount:], digits['target'][amount:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenaremos un modelo que usa **SGD** o **Stochastic Gradiant Descent** para clasificar por medio de la minimización del error que produce el mismo modelo.\n",
    "\n",
    "Es importante que para entrenar el modelo **sólo usemos los datos de entrenamiento**. La idea es que el clasificador no pueda ver los datos con los que lo probaremos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(max_iter=1000)\n",
    "\n",
    "sgd.fit(training_data, training_labels)\n",
    "\n",
    "print('Score modelo con datos de entrenamiento: {}\\n'.format(sgd.score(training_data, training_labels)))\n",
    "\n",
    "print('Score modelo con datos de test: {}\\n'.format(sgd.score(test_data, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número que nos entrega el método score nos dice que tan bueno es nuestro modelo para predecir. Como pueden ver, el score es muy cercano a 1 cuando lo probamos con datos de entrenamiento, esto es porque el modelo \"ya conoce\" los datos de entrenamiento, por lo que es importante probarlo con datos que no haya visto antes. Es por esto que para el score usamos los datos de testing que separamos anteriormente.\n",
    "\n",
    "Ahora veremos una **matriz de confusión**, la cual nos permite visualizar los resultados del modelo.\n",
    "\n",
    "En una matriz de confusión, el eje X corresponde a la clase verdadera de un dato, mientras que el eje Y corresponde a la clase predecida por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = sgd.predict(test_data)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_labels, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar Altair para visualizar mejor la matriz, y vemos que la diagonal posee colores mas claros, significando un mayor valor en ese elemento.\n",
    "\n",
    "La diagonal corresponde a todos los valores predichos que tienen el mismo valor real. Es decir que estan clasificados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(range(0, 10), range(0, 10))\n",
    "z = confusion_matrix(test_labels, test_predict)\n",
    "\n",
    "data = pd.DataFrame({'x': x.ravel(),\n",
    "                     'y': y.ravel(),\n",
    "                     'z': z.ravel()})\n",
    "\n",
    "alt.Chart(data).mark_rect().encode(\n",
    "    x='x:O',\n",
    "    y='y:O',\n",
    "    color='z:Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, clasificaremos por medio de una regresión logística multiclase, la cual usa múltiples regresiones logísticas para clasificar cada clase.\n",
    "\n",
    "Una regresión normal separa los datos en dos, por lo que de deben hacer varias para poder clasificar múltiples clases.\n",
    "\n",
    "Este modelo recibe varios parámetros, sin embargo usaremos el valor de **tol** o **tolerancia**, el cual mide la tolerancia del modelo frente a los errores.\n",
    "\n",
    "Por defecto toma un valor de 10^4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_n = LogisticRegression()\n",
    "\n",
    "logistic_n.fit(training_data, training_labels)\n",
    "\n",
    "logistic_n.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos un segundo modelo, con mucho mayor tolerancia a los errores.\n",
    "\n",
    "Podemos ver, al entrenarlo, que el score es bastante menor. Esto es debido a que, al llegar a un valor tolerable, el modelo deja de mejorar y termina. Como la tolerancia es mayor, el resultado no es tan bueno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(tol=3)\n",
    "\n",
    "logistic.fit(training_data, training_labels)\n",
    "\n",
    "logistic.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veremos las matrices de confusión de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_1 = logistic_n.predict(test_data)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_labels, test_predict_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_2 = logistic.predict(test_data)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_labels, test_predict_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el segundo modelo tiene una matriz de confusión mucho menos diagonal, significando muchos errores de clasificación.\n",
    "\n",
    "Para visualizar mejor, nuevamente mostraremos las matrices por medio de altair. Podemos observar de mejor manera la diferencia de los modelos frente a un cambio de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(range(0, 10), range(0, 10))\n",
    "z_1 = confusion_matrix(test_labels, test_predict_1)\n",
    "z_2 = confusion_matrix(test_labels, test_predict_2)\n",
    "\n",
    "data_1 = pd.DataFrame({'x': x.ravel(),\n",
    "                       'y': y.ravel(),\n",
    "                       'z': z_1.ravel(),\n",
    "                       'd': 'tol = 1e-4'})\n",
    "\n",
    "data_2 = pd.DataFrame({'x': x.ravel(),\n",
    "                       'y': y.ravel(),\n",
    "                       'z': z_2.ravel(),\n",
    "                       'd': 'tol = 3'})\n",
    "\n",
    "total_data = pd.concat([data_1, data_2])\n",
    "\n",
    "alt.Chart(total_data).mark_rect().encode(\n",
    "    column='d',\n",
    "    x='x:O',\n",
    "    y='y:O',\n",
    "    color='z:Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, clasificaremos usando un modelo de KNN, el cual dado un vector, busca los K vectores mas cercanos y toma la desición dependiendo de la mayor cantidad de vecinos iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_1 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn_1.fit(training_data, training_labels)\n",
    "\n",
    "knn_1.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_1_predict = knn_1.predict(test_data)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_labels, knn_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(range(0, 10), range(0, 10))\n",
    "z_1 = confusion_matrix(test_labels, knn_1_predict)\n",
    "\n",
    "data = pd.DataFrame({'x': x.ravel(),\n",
    "                       'y': y.ravel(),\n",
    "                       'z': z_1.ravel()})\n",
    "\n",
    "alt.Chart(data).mark_rect().encode(\n",
    "    x='x:O',\n",
    "    y='y:O',\n",
    "    color='z:Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_2 = KNeighborsClassifier(n_neighbors=100)\n",
    "\n",
    "knn_2.fit(training_data, training_labels)\n",
    "\n",
    "knn_2_predict = knn_2.predict(test_data)\n",
    "\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=500)\n",
    "\n",
    "knn_3.fit(training_data, training_labels)\n",
    "\n",
    "knn_3_predict = knn_3.predict(test_data)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(range(0, 10), range(0, 10))\n",
    "z_2 = confusion_matrix(test_labels, knn_2_predict)\n",
    "z_3 = confusion_matrix(test_labels, knn_3_predict)\n",
    "\n",
    "data_1 = pd.DataFrame({'x': x.ravel(),\n",
    "                       'y': y.ravel(),\n",
    "                       'z': z_1.ravel(),\n",
    "                       'd': 'k = 5'})\n",
    "\n",
    "data_2 = pd.DataFrame({'x': x.ravel(),\n",
    "                       'y': y.ravel(),\n",
    "                       'z': z_2.ravel(),\n",
    "                       'd': 'k = 20'})\n",
    "\n",
    "data_3 = pd.DataFrame({'x': x.ravel(),\n",
    "                       'y': y.ravel(),\n",
    "                       'z': z_3.ravel(),\n",
    "                       'd': 'k = 500'})\n",
    "\n",
    "total_data = pd.concat([data_1, data_2, data_3])\n",
    "\n",
    "alt.Chart(total_data).mark_rect().encode(\n",
    "    column='d',\n",
    "    x='x:O',\n",
    "    y='y:O',\n",
    "    color='z:Q'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
